# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/139xiMgHoJEdwdI3QFFXMJliZclkgeCZ1
"""

!pip install -q pandas numpy tqdm regex emoji langdetect deep-translator afinn transformers optimum[onnxruntime]

import os, re, time, sqlite3, warnings
from datetime import datetime
from typing import Dict
import pandas as pd
import numpy as np
from tqdm import tqdm
import emoji

from langdetect import detect, DetectorFactory
DetectorFactory.seed = 42

from deep_translator import GoogleTranslator

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
warnings.filterwarnings("ignore")
nltk.download("stopwords", quiet=True)
nltk.download("wordnet", quiet=True)

# ===================== CONFIG =====================
INPUT_CSV     = "reviews.csv"
DB_PATH       = "airbnb_reviews.db"
TABLE_NAME    = "reviews_sentiment"
SAMPLE_SIZE   = 50_000          # sample this many rows (from ALL languages)
BATCH_SIZE    = 256
MAX_LEN       = 128
CONF_THRESH   = 5               # AFINN |score| >= threshold => confident
SAVE_SAMPLE   = True            # saves the sampled raw rows (pre-translation)
MODEL_NAME    = "distilbert-base-uncased-finetuned-sst-2-english"

# performance hints (optional)
os.environ.setdefault("OMP_NUM_THREADS", "4")
os.environ.setdefault("MKL_NUM_THREADS", "4")

# ===================== Data Cleaning =====================
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text: str) -> str:
    if not isinstance(text, str): return ""
    text = re.sub(r"<.*?>", " ", text)
    text = re.sub(r"[@#]\w+", " ", text)
    text = emoji.replace_emoji(text, replace=" ")
    text = re.sub(r"[^a-zA-Z]", " ", text)
    toks = [lemmatizer.lemmatize(t) for t in text.lower().split() if t not in stop_words]
    return " ".join(toks)

def valid_text(text: str) -> bool:
    return isinstance(text, str) and text.strip() != ""

def detect_lang_safe(text: str) -> str:
    try:
        return detect(text) if isinstance(text, str) and text.strip() else "unknown"
    except Exception:
        return "unknown"

# ===================== Translating Foreign Language =====================
_TRANSLATOR = GoogleTranslator(source="auto", target="en")

def translate_text_safe(text: str, max_retries: int = 3, backoff_s: float = 1.0) -> str:
    if not isinstance(text, str) or not text.strip():
        return text
    for attempt in range(1, max_retries + 1):
        try:
            return _TRANSLATOR.translate(text)
        except Exception:
            if attempt == max_retries:
                return text
            time.sleep(backoff_s * attempt)

def translate_series_dedup(series: pd.Series) -> pd.Series:
    vals = series.dropna().unique().tolist()
    mapping: Dict[str, str] = {}
    for s in tqdm(vals, desc="Translating non-English"):
        mapping[s] = translate_text_safe(s)
    return series.map(mapping)

# ===================== Categorize Negative Reviews =====================
def categorize_reason(text: str) -> str:
    if not isinstance(text, str): return "Other"
    t = text.lower()
    if any(k in t for k in ["dirty","filthy","smell","odor","stain","mold","mould","hygiene","dust","bugs","bedbug"]):
        return "Cleanliness"
    if any(k in t for k in ["host","communication","respond","response","reply","message","unhelpful","rude","support"]):
        return "Communication"
    if any(k in t for k in ["bed","sofa","mattress","pillow","kitchen","tv","wifi","wi fi","heater","heating","ac","aircon","air con","appliance","furniture","shower","hot water","water pressure"]):
        return "Amenities"
    if any(k in t for k in ["location","far","distance","neighborhood","neighbourhood","area","transport","subway","unsafe"]):
        return "Location"
    if any(k in t for k in ["price","expensive","value","cost","overpriced","not worth"]):
        return "Value"
    if any(k in t for k in ["check in","check-in","checkin","check out","checkout","arrival","key","lockbox","code","instructions"]):
        return "Check-in"
    if any(k in t for k in ["noise","noisy","loud","music","construction","thin walls","party"]):
        return "Noise"
    if any(k in t for k in ["cancellation","cancel","overbook","over-book","refund","deposit","policy"]):
        return "Booking/Policy"
    return "Other"

# ===================== PIPELINE =====================
def main():
    # [1/10] LOAD (full file)
    print("[1/10] Loading CSV …")
    try:
        df = pd.read_csv(INPUT_CSV, low_memory=False)
        total_rows = len(df)
        print(f"  └─ Loaded {total_rows:,} rows from {INPUT_CSV}")
        print(f"  └─ Columns: {list(df.columns)}")
    except FileNotFoundError:
        raise FileNotFoundError(f"Input file not found: {INPUT_CSV}")
    except Exception as e:
        raise RuntimeError(f"Error loading CSV: {e}")

    # [2/10] BASIC NORMALIZE (lightweight; no language work here)
    print("\n[2/10] Basic normalization …")
    rename_map = {}  # adjust if needed
    df = df.rename(columns=rename_map)

    if "comments" not in df.columns:
        raise ValueError("Missing required column: 'comments'")

    if "id" not in df.columns:
        df["id"] = np.arange(len(df)).astype(str)

    df["comments"] = df["comments"].astype(str)

    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.strftime("%Y-%m-%d")

    before_dupes = len(df)
    df = df.drop_duplicates().drop_duplicates(subset=["id"], keep="first")
    print(f"  └─ Dropped duplicates: {before_dupes - len(df):,}")
    print(f"  └─ Rows after basic normalize: {len(df):,}")

    # [3/10] SAMPLE FIRST (fast) — from ALL languages
    print("\n[3/10] Sampling …")
    n = min(SAMPLE_SIZE, len(df))
    sample = df.sample(n=n, random_state=42).copy()
    print(f"  └─ Sample size: {len(sample):,} (of {len(df):,} total rows)")

    if SAVE_SAMPLE:
        raw_sample_path = "reviews_sample_raw_50k.csv"
        sample.to_csv(raw_sample_path, index=False)
        print(f"  └─ Saved raw sample CSV → {raw_sample_path}")

    # [4/10] LANGUAGE DETECTION on the SAMPLE ONLY (fast)
    print("\n[4/10] Detecting language on sample …")
    tqdm.pandas(desc="langdetect(sample)")
    sample["Language"] = sample["comments"].progress_map(detect_lang_safe)
    print("  └─ Language breakdown (top 10):")
    print(sample["Language"].value_counts(dropna=False).head(10).to_string())

    # [5/10] TRANSLATE non-English in sample -> comments_en; CLEAN -> comments_clean
    print("\n[5/10] Auto-translate non-English & clean …")
    sample["is_english"] = sample["Language"].eq("en")
    sample["comments_en"] = sample["comments"]  # default: original
    non_en_mask = ~sample["is_english"]
    if non_en_mask.any():
        sample.loc[non_en_mask, "comments_en"] = translate_series_dedup(sample.loc[non_en_mask, "comments"])
        print(f"  └─ Translated rows: {int(non_en_mask.sum()):,}")
    else:
        print("  └─ No non-English rows in sample.")

    sample["comments_clean"] = sample["comments_en"].map(clean_text)
    sample["tokens_len"] = sample["comments_clean"].str.split().map(len)

    valid_mask = sample["comments_clean"].map(valid_text)
    sample = sample[valid_mask].copy()
    print(f"  └─ Valid rows after translation + clean: {len(sample):,}")

    # [6/10] SENTIMENT: AFINN fast-pass + Transformer on ambiguous
    print("\n[6/10] AFINN fast-pass sentiment …")
    from afinn import Afinn
    af = Afinn()
    sample["afinn_score"] = pd.to_numeric(sample["comments_clean"].map(af.score), errors="coerce").fillna(0).astype(int)

    conf_mask = sample["afinn_score"].abs().ge(CONF_THRESH)
    conf = sample.loc[conf_mask].copy()
    amb  = sample.loc[~conf_mask].copy()

    conf["sentiment_label"] = np.where(conf["afinn_score"] >= 0, "POSITIVE", "NEGATIVE")
    cap = 10
    conf["sentiment_score"] = (conf["afinn_score"].clip(-cap, cap) + cap) / (2 * cap)

    print(f"  └─ AFINN confident rows: {len(conf):,}")
    print(f"  └─ To model (ambiguous): {len(amb):,}")

    used_model = False
    if len(amb) > 0:
        print("\n[6b/10] Transformer (ORT) on ambiguous rows …")
        try:
            from transformers import AutoTokenizer
            from optimum.onnxruntime import ORTModelForSequenceClassification
            import torch

            tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
            ort_model = ORTModelForSequenceClassification.from_pretrained(MODEL_NAME)
            id2label = ort_model.config.id2label

            texts = amb["comments_clean"].tolist()
            tok_kwargs = dict(truncation=True, max_length=MAX_LEN, padding=True, return_tensors="pt")

            labels_out, scores_out = [], []
            for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="ORT inference"):
                batch = texts[i:i + BATCH_SIZE]
                tokens = tokenizer(batch, **tok_kwargs)
                with torch.no_grad():
                    logits = ort_model(**tokens).logits
                    probs = torch.softmax(logits, dim=1)
                    confv, pred = probs.max(dim=1)
                labels_out.extend([id2label[int(j)] for j in pred])
                scores_out.extend(confv.detach().cpu().numpy().tolist())

            amb["sentiment_label"] = labels_out
            amb["sentiment_score"] = scores_out
            used_model = True
            print("  └─ Transformer inference complete.")
        except Exception as e:
            print(f"  └─ Transformer unavailable ({e}). Falling back to AFINN sign for ambiguous.")
            amb["sentiment_label"] = np.where(amb["afinn_score"] >= 0, "POSITIVE", "NEGATIVE")
            amb["sentiment_score"] = 0.5

    # [7/10] COMBINE + NEGATIVE REASON
    print("\n[7/10] Combining & categorizing negative reasons …")
    out = pd.concat([conf, amb], axis=0).sort_index()
    out["negative_reason"] = np.where(
        out["sentiment_label"] == "NEGATIVE",
        out["comments_clean"].map(categorize_reason),
        None
    )

    print("  └─ Sentiment distribution:")
    with pd.option_context("display.max_rows", None):
        print(out["sentiment_label"].value_counts(dropna=False).to_string())

    neg = out[out["sentiment_label"] == "NEGATIVE"]
    if len(neg):
        print("\n  └─ Top negative reasons:")
        print(neg["negative_reason"].value_counts().head(10).to_string())

    # [8/10] PREVIEW
    print("\n[8/10] Preview (first 8 rows):")
    preview_cols = ["id"]
    if "date" in out.columns: preview_cols.append("date")
    preview_cols += ["Language", "comments", "comments_en", "comments_clean",
                     "afinn_score", "sentiment_label", "sentiment_score", "negative_reason"]
    with pd.option_context("display.max_columns", None, "display.width", 220):
        print(out[preview_cols].head(8).to_string(index=False))

    # [9/10] WRITE TO SQLITE
    print("\n[9/10] Writing to SQLite …")
    conn = sqlite3.connect(DB_PATH)
    try:
        conn.execute("PRAGMA journal_mode=WAL;")
        conn.execute("PRAGMA synchronous=OFF;")
        conn.execute("PRAGMA temp_store=MEMORY;")
        conn.execute("PRAGMA mmap_size=30000000000;")
    except Exception:
        pass

    out.to_sql(TABLE_NAME, conn, index=False, if_exists="replace")

    # Helpful indexes
    try:
        conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_id    ON {TABLE_NAME}(id);")
        if "date" in out.columns:
            conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_date  ON {TABLE_NAME}(date);")
        conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{TABLE_NAME}_lang  ON {TABLE_NAME}(Language);")
        conn.commit()
    except Exception:
        pass

    conn.close()
    print(f"  └─ Saved {len(out):,} rows to {DB_PATH} (table: {TABLE_NAME})")

    # [10/10] RUN METADATA
    print("\n[10/10] Run metadata …")
    meta = {
        "run_ts": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "input_csv": os.path.abspath(INPUT_CSV),
        "total_input_rows": int(total_rows),
        "sample_size": int(len(sample)),
        "processed_rows": int(len(out)),
        "model_name": MODEL_NAME,
        "engine": f"hybrid(afinn+{'ort' if used_model else 'fallback'}) on English (original/translated)",
        "batch_size": int(BATCH_SIZE),
        "max_length": int(MAX_LEN),
        "afinn_conf_thresh": int(CONF_THRESH),
        "db_path": os.path.abspath(DB_PATH),
        "table_name": TABLE_NAME
    }
    for k, v in meta.items():
        print(f"  • {k}: {v}")

    print("\n✅ Done.")

if __name__ == "__main__":
    main()

